# EvidenzLLM Web Chat

A modern web-based question answering system that uses evidence-based retrieval and Google Gemini to provide accurate, well-supported answers with a clean ChatGPT-like interface.

## âœ¨ Features

- **ğŸ¯ Smart Query Classification**: Automatically categorizes questions (factual, explanation, reasoning, calculation)
- **ğŸ” Hybrid Retrieval**: Combines BM25, dense embeddings, and cross-encoder reranking
- **ğŸ“š Evidence-Based Answers**: Generates answers using Google Gemini with retrieved Wikipedia evidence
- **ğŸ’¬ Modern Chat UI**: Clean, responsive interface with light effects and smooth animations
- **ğŸ·ï¸ Topic Tags**: Displays available Wikipedia topics in the header
- **âš¡ Fast & Reliable**: Optimized pipeline with proper error handling

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Google Gemini API key ([Get one here](https://makersuite.google.com/app/apikey))

### Installation

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Download NLTK data
python -c "import nltk; nltk.download('punkt')"

# 3. Configure environment
cp .env.example .env
# Edit .env and add your GOOGLE_API_KEY

# 4. Generate Wikipedia knowledge base
python prepare_data.py
```

### Download Pre-Trained Classifier
You need the query classifier model from the original EvidenzLLM notebook. Place it as `query_classifier_model/` in this directory.

Get a pretrained classifier [here](https://drive.google.com/drive/folders/11eLqNQHzvGy6KBFfqlZJigcynLETLNdH?usp=sharing)

### Run

```bash
# Start the server
python app.py

# Or use the startup script (handles environment variables)
./start_server.sh
```

Open http://localhost:5000 in your browser and start asking questions!

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py                      # Flask backend server
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ pipeline.py            # Main pipeline orchestrator
â”œâ”€â”€ models/
â”‚   â””â”€â”€ models.py              # Query classifier model
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ retrieval.py           # Hybrid retrieval system
â”œâ”€â”€ generator/
â”‚   â””â”€â”€ generator.py           # Gemini API integration
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html             # Chat interface
â”‚   â”œâ”€â”€ style.css              # Modern styling
â”‚   â””â”€â”€ app.js                 # Frontend logic
â”œâ”€â”€ query_classifier_model/    # Pre-trained classifier (not in repo)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ wiki_texts.pkl         # Wikipedia knowledge base (not in repo)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env.example               # Example configuration
â””â”€â”€ README.md                  # This file
```

## âš™ï¸ Configuration

Edit `.env` to customize:

```bash
# Required
GOOGLE_API_KEY=your-api-key-here

# Optional (with defaults)
CLASSIFIER_PATH=./query_classifier_model
WIKI_DATA_PATH=./data/wiki_texts.pkl
GEMINI_MODEL=gemini-2.0-flash
PORT=5000
```

## ğŸ¨ Interface

- **Topic Tags**: Shows available Wikipedia topics in rounded gray pills
- **Query Type Badges**: Displays the classified query type
- **Evidence Passages**: Shows supporting evidence with dark gray borders

## ğŸ”Œ API Endpoints

### GET /api/health
Health check endpoint

### GET /api/topics
Get available Wikipedia topics

### POST /api/query
Process a question

**Request:**
```json
{
  "question": "What is machine learning?"
}
```

**Response:**
```json
{
  "question": "What is machine learning?",
  "query_type": "explanation",
  "answer": "Machine learning is...",
  "passages": [...]
}
```

## ğŸ§ª Example Questions

- **Factual**: "Who discovered gravity?"
- **Explanation**: "What is machine learning?"
- **Reasoning**: "Why does the sky appear blue?"
- **Calculation**: "Calculate 15% of 200"

## ğŸ› ï¸ Troubleshooting

### macOS Threading Error
```bash
export TOKENIZERS_PARALLELISM=false
export MKL_THREADING_LAYER=GNU
python app.py
```

Or just use `./start_server.sh` which handles this automatically.


### API Key Issues
- Verify your key in `.env` file
- Check it's active at https://makersuite.google.com/app/apikey
- Ensure Gemini API is enabled

For more troubleshooting, see [SETUP.md](SETUP.md).

## ğŸ“Š Performance

- **First request**: 3-5 seconds (model loading)
- **Subsequent requests**: 2-4 seconds
- **Memory**: ~4GB RAM recommended
- **GPU**: Automatically used if available

## ğŸ”’ Security Notes

- Never commit `.env` file (contains API key)
- Model files and data are excluded from git
- API key is required but kept secure

## ğŸ“ License

This project is based on the EvidenzLLM notebook and uses various open-source models and libraries.

## ğŸ™ Acknowledgments

- Query Classifier: Fine-tuned DeBERTa model
- Dense Retrieval: sentence-transformers multi-qa-mpnet-base-dot-v1
- Cross-Encoder: ms-marco-MiniLM-L-6-v2
- Answer Generation: Google Gemini
- Knowledge Base: Wikipedia

---

**Need help?** Check [SETUP.md](SETUP.md) for detailed setup instructions and troubleshooting.
