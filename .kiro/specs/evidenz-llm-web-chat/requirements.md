# Requirements Document

## Introduction

This feature transforms the existing EvidenzLLM Colab notebook pipeline into a production-ready web chat interface. The system will enable users to ask questions through a web UI, where queries are classified, enriched with evidence from a Wikipedia knowledge base using hybrid retrieval (BM25 + dense embeddings + cross-encoder reranking), and answered using Google Gemini Pro API. The interface will display both the generated answer and the supporting evidence passages with their relevance scores.

## Requirements

### Requirement 1: Query Classification System

**User Story:** As a user, I want my questions to be automatically classified by type, so that the system can handle different query types appropriately.

#### Acceptance Criteria

1. WHEN a user submits a question THEN the system SHALL tokenize the question using the DeBERTa tokenizer
2. WHEN the tokenized question is processed THEN the system SHALL load the pre-trained QueryClassifier model from the `query_classifier_model` directory
3. WHEN the model processes the input THEN the system SHALL classify the query into one of four types: factual_lookup, explanation, reasoning, or calculation
4. WHEN classification is complete THEN the system SHALL return the predicted query type label
5. IF the model files are missing or corrupted THEN the system SHALL return an error message indicating the model cannot be loaded

### Requirement 2: Hybrid Evidence Retrieval System

**User Story:** As a user, I want the system to find the most relevant evidence from Wikipedia, so that answers are well-supported and accurate.

#### Acceptance Criteria

1. WHEN the system initializes THEN it SHALL load the Wikipedia text chunks and metadata from storage
2. WHEN the system starts THEN it SHALL initialize BM25, dense embedding model (multi-qa-mpnet-base-dot-v1), FAISS index, and cross-encoder (ms-marco-MiniLM-L-6-v2)
3. WHEN a query is received THEN the system SHALL retrieve top candidates using BM25 scoring
4. WHEN BM25 candidates are identified THEN the system SHALL retrieve top candidates using dense embeddings and FAISS search
5. WHEN both candidate sets are available THEN the system SHALL compute hybrid scores using weighted combination (alpha=0.6)
6. WHEN hybrid candidates are ranked THEN the system SHALL rerank the top 32 candidates using the cross-encoder
7. WHEN reranking is complete THEN the system SHALL return the top 5 passages with their titles, text chunks, and cross-encoder scores
8. IF the Wikipedia data or indices are missing THEN the system SHALL return an error indicating retrieval is unavailable

### Requirement 3: Answer Generation via Gemini API

**User Story:** As a user, I want to receive accurate answers generated by Google Gemini Pro based on retrieved evidence, so that I get reliable information.

#### Acceptance Criteria

1. WHEN evidence passages are retrieved THEN the system SHALL construct a RAG prompt including system instructions, few-shot example, question, query type, and numbered evidence snippets
2. WHEN the prompt is constructed THEN the system SHALL send it to the Google Gemini Pro API
3. WHEN calling the API THEN the system SHALL include the API key from environment variables or configuration
4. WHEN the API responds THEN the system SHALL extract the generated answer text
5. WHEN the answer is received THEN the system SHALL return both the answer and the evidence passages used
6. IF the API call fails THEN the system SHALL return an error message with details about the failure
7. IF the API key is missing or invalid THEN the system SHALL return an authentication error

### Requirement 4: Web Chat Interface

**User Story:** As a user, I want to interact with the system through a clean web chat interface, so that I can easily ask questions and view answers.

#### Acceptance Criteria

1. WHEN a user accesses the application THEN the system SHALL display a chat interface with an input field and message history
2. WHEN a user types a question and submits THEN the system SHALL display the question in the chat history
3. WHEN processing begins THEN the system SHALL show a loading indicator
4. WHEN an answer is generated THEN the system SHALL display the answer in the chat history
5. WHEN an answer is displayed THEN the system SHALL show the 5 supporting evidence passages below the answer
6. WHEN evidence is displayed THEN each passage SHALL show its title, cross-encoder score, and text preview
7. WHEN a user submits multiple questions THEN the system SHALL maintain conversation history in the interface
8. IF an error occurs THEN the system SHALL display a user-friendly error message in the chat

### Requirement 5: Backend API Service

**User Story:** As a developer, I want a REST API that handles query processing, so that the frontend can communicate with the ML pipeline.

#### Acceptance Criteria

1. WHEN the server starts THEN it SHALL load all required models and indices into memory
2. WHEN the server receives a POST request to /api/query THEN it SHALL accept a JSON payload with a "question" field
3. WHEN a valid query is received THEN the system SHALL execute the full pipeline: classification, retrieval, and generation
4. WHEN processing is complete THEN the system SHALL return a JSON response containing the answer, query_type, and evidence passages
5. WHEN an error occurs during processing THEN the system SHALL return an appropriate HTTP error code and error message
6. IF the server is not ready THEN health check endpoint SHALL return service unavailable status

### Requirement 6: Configuration and Environment Management

**User Story:** As a developer, I want to configure the application through environment variables, so that sensitive credentials and settings are managed securely.

#### Acceptance Criteria

1. WHEN the application starts THEN it SHALL load configuration from environment variables or a .env file
2. WHEN loading configuration THEN the system SHALL require GOOGLE_API_KEY for Gemini API access
3. WHEN loading configuration THEN the system SHALL support optional parameters for model paths, retrieval settings (top_k, alpha), and server port
4. IF required environment variables are missing THEN the system SHALL log an error and fail to start
5. WHEN configuration is loaded THEN sensitive values SHALL NOT be logged or exposed in responses

### Requirement 7: Model and Data Persistence

**User Story:** As a developer, I want the system to use the pre-trained models and Wikipedia data from the notebook, so that the web application maintains the same quality.

#### Acceptance Criteria

1. WHEN the application initializes THEN it SHALL load the QueryClassifier model from the `query_classifier_model` directory
2. WHEN loading the classifier THEN the system SHALL support both safetensors and pytorch_model.bin formats
3. WHEN the application initializes THEN it SHALL load or build Wikipedia chunks, BM25 index, and FAISS dense index
4. WHEN Wikipedia data is not available THEN the system SHALL provide instructions for generating the data
5. IF model files are incompatible THEN the system SHALL log detailed error information

### Requirement 8: Performance and Resource Management

**User Story:** As a user, I want the system to respond quickly to my questions, so that I have a smooth experience.

#### Acceptance Criteria

1. WHEN models are loaded THEN the system SHALL use GPU acceleration if available, otherwise CPU
2. WHEN processing a query THEN the classification step SHALL complete within 1 second
3. WHEN processing a query THEN the retrieval step SHALL complete within 2 seconds
4. WHEN calling Gemini API THEN the system SHALL implement a timeout of 30 seconds
5. WHEN multiple requests arrive THEN the system SHALL handle them concurrently without blocking
